首要：先只跑3w的最小文件，看看是否运行成功，速度快一点。

在data文件夹里面，pkl文件是钓鱼节点出发的二阶图，txt文件是钓鱼节点。
在代码文件里面，有一部分代码和本论文无关，不过很少，里面有随机游走的代码，可以从pkl文件里面提取3 4 5w子图。
里面也有特征嵌入的文件。



代码文件有很多，运行的时候需要有顺序，即论文中的数据处理的顺序（很重要，需要先明白文件先后顺序，因为会生成很多文件，而这些文件是其他文件的输入数据。）
MHRW是Metropolis-Hastings Random Walk方法的缩写，是一种随机游走算法，MHRW 经常用于网络分析和抽样，特别是用于从大型网络中进行随机游走，以便收集样本或进行数据分析。

1:其中目前已知不属于本论文的文件是：subgraph,这个文件只是想看看Graph_4942.pkl的最大连通子图

2:运行randomwalk代码，可以跑出具有 3w 4 w 5w 个节点的三个文件。

3:在文件MHRW.py中，使用了Graph_4942.pkl文件以及feature_all.csv，其中feature_all.csv文件这是一个CSV文件，包含了节点特征数据。该文件用于读取节点特征信息，并将其存储在一个字典中，用于节点特征的计算。运行这个代码会生成新文件node_MHRW_50000.txt。
这是一个文本文件，用于存储随机游走算法生成的节点序列。每一行包含一个节点标识。这个文件记录了随机游走过程中访问的节点，带有节点特征信息。这是生成的新文件，存储了随机游走的结果。
因此，这段代码使用了已有的图形数据和节点特征数据，生成了新的文本文件以存储随机游走的结果猜想：）

4：feature.py文件功能是计算和提取网络图中节点的各种特征，并将这些特征保存到一个CSV文件中，以便后续的数据分析和处理。这些特征包括节点的度、交易金额、邻居数量等，用于分析网络中的节点行为和特性。参考的是GCN那篇论文（但是呢，TTAGN中用的是10个特征，代码中是8个)

5:transaction.py主要功能是从一个已有的图形数据中筛选出特定节点的子图，并提取该子图中的交易数据，然后将这些交易数据保存到一个CSV文件中，以便后续的数据分析和处理。这段代码输入用到了两个文件：Graph_4942.pkl和node_MHRW_40000.txt（这是一个文本文件，包含了随机游走算法生成的节点标识。代码从这个文件中读取节点标识，以便筛选子图。）最后生成了transaction40000_MHRW.csv，包含了从筛选后的子图中提取的交易数据。文件中的列包括起始节点、目标节点、交易金额和时间戳等属性。这个文件保存了子图中的交易数据，可用于后续的数据分析和处理。

6:跑完features.py文件之后再跑MHRW.py文件。

7:在sub_analysis.py文件中，这段代码的主要作用是从两个数据文件中提取特定节点的特征信息，然后将这些特征信息保存到一个新的CSV文件中。其中输入是Random walk文件夹下面的node50000.txt文件，输出的文件保存在Random walk文件夹下面，命名为subgraph_50000.csv


11.05：随机游走用randomwalk.py文件，不用MHRW.py，其他代码中的输入文件node_MHRW_30000.txt换成node_30000.txt就行了。当时师兄试图修改随机游走方法，所以才用了MHRW文件。

8:先跑的sub_analysis.py生成了subgraph_30000.csv，然后这个subgraph_30000.csv和phish_hack_nodes.txt文件作为输入跑label.py文件，后面会生成label_30000.csv

9:subgraph.py文件主要是从一个大图中提取其最大弱连通子图，并将该子图保存到subgraph.pkl文件中，不过这个文件功能在TTAGN中没有体现，应该是GCN那篇论文中的步骤。（11.05  14.13）

10：adjacency_matrix.py这个文件中的输入文件还未产生，目前不知道这个代码是否属于TTAGN。

11：dataload.py这个文件是比较靠后运行的文件，这个文件的输入需要很多文件。

12：data_process.py文件很重要,输入文件用到了label_30000.csv和transaction30000.csv文件，这个代码首先会从label_30000.csv里面读取节点数据，将节点的标签信息存储为‘node’的列表中，然后从transaction30000.csv文件中获取交易数据，从交易数据中
提取节点的标签，然后根据节点标签在 node 列表中查找对应的节点索引，并将该索引与交易数据中的其他信息（如时间戳和交易金额）一起写入到名为 transaction30000_1.csv 的新CSV文件中。transaction30000_1.csv会作为dataload.py的输入文件。

11.6

13：lstm.py文件会调用layer.py文件，调用时频繁出现错误，改了挺久，后来可以了，运行成功生成了GCN文件用于lgbmc.py文件作为输入。(记录貌似有错误)
14:exc.py代码用于生成train_mask 和 test_mask，用于将数据集分成训练集和测试集，并更新CSV文件以包含这些信息